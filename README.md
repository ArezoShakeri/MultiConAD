# MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer’s Detection
This repository contains the code and data processing pipeline for our paper, MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer’s Detection. The repository includes scripts for data collection, preprocessing, transcription, and experimentation.

## Repository Structure
Audio transcription/ ASR_audio_dataset.py – Transcribes audio data.

Experiments/TF_IDF_classifier.py, e5_larg_classifier.py – Contains code for multiclass and binary classification tasks using TF-IDF and E5 Large model experiments for monolingual, combined-multilingual and combined-translated settings.

Extracting data/ collection.py, cha.collection.py, TSV.collection.py, ASR_collection.py, test_ch_collection.py – Scripts for collecting data from various 16 datasets used in this study and putting them in the normalized class format defined in collection.py script.
cha.collection.py: Used for collecting text data from dementiaBank Chat format files.
TSV.collection.py: Uesd only for text-based data for the iFlytek dataset.
ASR_collection.py: Used for organizing transcripts of audio-based data which is generated by Whisper.
test_ch_collection.py: This is a sample script to test the output of the normalized dataset for the chat format file.

Preprocessing_text/text_cleaning_Chinese.py, text_cleaning_English.py, text_cleaning_Greek.py, text_cleaning_Spanish.py: Used to preprocess text in each language.
Translation/translation_all_language.py: Create a translation column for all non-English datasets and add the translation.

Translation/translation_all_language.py: Translating all non-English data to English using GPT4.



### Audio Transcription:
Transcribed audio files using ASR_audio_dataset.py.
Normalized and stored transcripts in the normalized folder using ASR_collection.py script.


### Extracting Data (Data Collection):
We gathered data from multiple sources using collection.py, cha.collection.py, TSV.collection.py, and ASR_collection.py.

### Preprocessing_text:
Combining datasets from each language and splitting the dataset to train and test split using text_cleaning_Chinese.py, text_cleaning_English.py, text_cleaning_Greek.py, text_cleaning_Spanish.py.
Each script processed each language separately.

### Translation
Created English translations for Greek, Spanish, and Chinese datasets.





## Experiments:

Conducted experiments using TF-IDF and E5 Large models: TF_IDF_classifier.py, e5_larg_classifier.py.
These scripts include code for multiclass and binary classification tasks utilizing TF-IDF and E5 Large model experiments across monolingual, combined-multilingual, and combined-translated settings.
